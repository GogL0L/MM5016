#+title: Lab 7
#+description: 
#+PROPERTY: header-args :tangle ./lab7.py :padline 2

* Header
#+begin_src python :results output :session :padline 0
## Description
"""
The column vector 'X' will be symbolised as a list of size 1 lists, that is
X = [[x1], [x2] ,[x3], ... , [xn]]. An m by n matrix 'A' will be symbolised by a list of size
m consisting of lists of size n. The solution to task 2 in the lab will be the function
'jacobi_method'.
"""

from math import sqrt
#+end_src

#+RESULTS:

* Linear algebra

** Linear algebra operation

*** matrix multiply
#+begin_src python :results output :session
def matrix_multiply(matrix1, matrix2):
    m = len(matrix1)
    n = len(matrix2)
    p = len(matrix2[0])
    element = lambda i,j: sum([matrix1[i][k] * matrix2[k][j]
                               for k in range(n)])
    row = lambda i: [element(i,j) for j in range(p)]
    return [row(i) for i in range(m)]
#+end_src

#+RESULTS:

*** Transpose
#+begin_src python :results output :session
def transpose(matrix):
    """ Transposes an n by m matrix. """
    n = len(matrix)
    m = len(matrix[0])
    return [[matrix[i][j] for i in range(n)] for j in range(m)]
#+end_src

#+RESULTS:

**** Test av transpose
#+begin_src python :results output :session :tangle no
matrix = [[1,2,3],[4,5,6],[7,8,9]]
vector = [[1],[2],[3]]
print("Matrix tranposed:", transpose(matrix))
print("Vector tranposed:", transpose(vector))
#+end_src

#+RESULTS:
: Matrix tranposed: [[1, 4, 7], [2, 5, 8], [3, 6, 9]]
: Vector tranposed: [[1, 2, 3]]

*** Dot product
#+begin_src python :results output :session
def dot_product(v,u):
    """ Takes the dot product of the vectors v and u. I.e. the inner product
    given that the coordinates are in a orthonormal basis.
    """
    n = len(v)
    vt = transpose(v)
    return matrix_multiply(vt,u)[0][0]
#+end_src

#+RESULTS:

**** Test av dot product
#+begin_src python :results output :session :tangle no
v = [[1], [2], [3]]
u = [[2], [1], [3]]
print(dot_product(v,u))
#+end_src

#+RESULTS:
: 13

*** matrix add
#+begin_src python :results output :session
def matrix_add(A,B):
    """ Add matrices componentwise. """
    n = len(A)
    m = len(A[0])
    return [[A[i][j] + B[i][j] for j in range(m)] for i in range(n)]
#+end_src

#+RESULTS:

**** test av matrix add
#+begin_src python :results output :session :tangle no
A = [[1,2], [3,4]]
B = [[2,4], [5,6]]
print(matrix_add(A,B))
#+end_src

#+RESULTS:
: [[3, 6], [8, 10]]

*** Scale
#+begin_src python :results output :session
def scale(l,A):
    """ Scales the matrix 'A' with the scalar m. """
    n = len(A)
    m = len(A[0])
    return [[l * A[i][j] for j in range(m)] for i in range(n)]
#+end_src

#+RESULTS:

**** Test av scalar product
#+begin_src python :results output :session :tangle no
A = [[1,2], [3,4]]
v = [[2], [3]]
print ("2 times A:", scale(-2, A))
print ("3 times v:", scale(3, v))
#+end_src

#+RESULTS:
: 2 times A: [[-2, -4], [-6, -8]]
: 3 times v: [[6], [9]]

*** matrix sub
#+begin_src python :results output :session
def matrix_sub(A,B):
    """ Add vectors componentwise. """
    negative_B = scale(-1, B)
    return matrix_add(A, negative_B)
#+end_src

#+RESULTS:

**** Test
#+begin_src python :results output :session :tangle no
A = [[1,2], [3,4]]
B = [[0,2], [4,1]]
print(matrix_sub(A,B))
#+end_src

#+RESULTS:
: [[1, 0], [-1, 3]]

*** Euclidean norm
#+begin_src python :results output :session
def norm(A):
    """ Returns the euclidean norm given the basis is orthonormal. """
    return sqrt(dot_product(A,A))
#+end_src

#+RESULTS:

**** test av euclidean norm
#+begin_src python :results output :session :tangle no
A = [[3], [0], [4]]
print(norm(A))
#+end_src

#+RESULTS:
: 5.0

*** Normalise
#+begin_src python :results output :session
def normalise(v):
    """ Normalises the vector 'v' using the euclidean norm. """
    return scale(1 / norm(v), v)
#+end_src

#+RESULTS:

**** Test of normalise
#+begin_src python :results output :session
v = [[ 0 ],[ 1 ],[ 1 ]]
print(normalise(v))
#+end_src

#+RESULTS:
: [[0.0], [0.7071067811865475], [0.7071067811865475]]

*** Projection
#+begin_src python :results output :session
def projection(u,v):
    """ Takes the projection of v on u, given that the coordinates
    are in a orthnormal basis.
    """
    scalar = dot_product(u,v) / dot_product(u,u)
    return scale(scalar, u)
    
#+end_src

#+RESULTS:

*** Test of above
#+begin_src python :results output :session :tangle no
u,v = [[ 3 ],[ 3 ]], [[ 5 ],[ 0 ]]
print(projection(v,u))
#+end_src

#+RESULTS:
: [[3.0], [0.0]]

** Related to jacobi method

*** simple decompose matrix
#+begin_src python :results output :session
def additive_decomposition(A):
    """ Decomposes an n by n matrix 'A' into an lower triangular
    matrix 'L', diagonal matrix 'U' and an upper triangular
    matrix 'U' such that 'A = D + L + U'. This function
    returns the 3 tuple '(D, L, U)'.
    """
    n = len(A)
    D = [[A[i][j] if i==j else 0 for j in range(n)] for i in range(n)]
    L = [[A[i][j] if i<j else 0 for j in range(n)] for i in range(n)]
    U = [[A[i][j] if i>j else 0 for j in range(n)] for i in range(n)]
    return D,L,U
#+end_src

#+RESULTS:

**** Test additive decomposition
#+begin_src python :results output :session :tangle no
A = [[1,2,3], [4,5,6], [7,8,9]]
D,L,U = additive_decomposition(A)
print("D:",D)
print("L:",L)
print("U:",U)
#+end_src

#+RESULTS:
: D: [[1, 0, 0], [0, 5, 0], [0, 0, 9]]
: L: [[0, 2, 3], [0, 0, 6], [0, 0, 0]]
: U: [[0, 0, 0], [4, 0, 0], [7, 8, 0]]

*** Diagonal inverse
#+begin_src python :results output :session
def diagonal_inverse(D):
    """ Given a diagonal n by n matrix 'D' with non-zero diagonals this function 
    returns the inverse matrix.
    """
    n = len(D)
    return [[1 / D[i][j] if i==j else 0 for j in range(n)] for i in range(n)]
#+end_src

#+RESULTS:

**** diagonal inverse test
#+begin_src python :results output :session :tangle no
D = [[1,0,0], [0, 4, 0], [0,0,10]]
print(diagonal_inverse(D))
#+end_src

#+RESULTS:
: [[1.0, 0, 0], [0, 0.25, 0], [0, 0, 0.1]]

*** Euclidean distance
#+begin_src python :results output :session
def euclidean_distance(v,u):
    """ Given the n by 1 vectors 'v' and 'u', this function will return
    the euclidean distance between them.
    """
    uv = matrix_sub(v,u)
    return norm(uv)
#+end_src

#+RESULTS:

**** test
#+begin_src python :results output :session :tangle no
u, v = [[0], [3], [0]], [[0], [0], [4]]
print(euclidean_distance(v,u))
#+end_src

#+RESULTS:
: 5.0

*** get zero matrix
#+begin_src python :results output :session
def zero_matrix(A):
    """ Returns the zero matrix of the same size. """
    n = len(A)
    m = len(A[0])
    return [[0 for j in range(m)] for i in range(n)]
#+end_src

#+RESULTS:

**** test
#+begin_src python :results output :session :tangle no
A = [[1,2], [3,4]]
v = [[1], [5]]
print("2x2 zero matrix:", zero_matrix(A))
print("2x1 zero matrix:", zero_matrix(v))
#+end_src

#+RESULTS:
: 2x2 zero matrix: [[0, 0], [0, 0]]
: 2x1 zero matrix: [[0], [0]]


* Iterate method

#+begin_src python :results output :session
## Iterate function
def iterate(function, root_approximate, output,
            tolerance, distance_metric, next_value_function,
            iteration=0, max_iterations=100, debug=False):
    """ Approximates a root x for the equation function(x) = output,
    by applying the next_value_function until tolerance is met by having
    distance_metric(function(root_approximate), output) <= tolerance. If debug 
    is True, then a tuple of that mentioned value and the iteration is returned. 
    An error is raised if iteration exceeds max_iterations.
    """
    # The root_approximates list will usually be (always, in the context of
    # this asignment) updated by calculating a new value based on all
    # the values in the root_approximates list, and then appending that new value,
    # and deleting the first element in root_approximates.
    # So the last element in root_approximates will be the most recent estimate of
    # the root. In the case of Newton's method, the root_approximates list
    # will just be a single value surrounded by a list.
    satisfies_tolerance = (
        distance_metric(function(root_approximate), output) <= tolerance)
    if iteration > max_iterations:
        raise RecursionError("<iterate: maximum ammount of iterations reached>")
    elif satisfies_tolerance and debug:
        return root_approximate, iteration
    elif satisfies_tolerance and not debug:
        return newest_root_approximate
    else:
        next_value = next_value_function(root_approximate)
        return iterate(function, next_values, tolerance, next_values_function,
                       iteration+1, max_iterations, debug)
#+end_src

#+RESULTS:

* Jacobi related

** Jacobi next value function
#+begin_src python :results output :session
def jacobi_next_value(X, D, L, U, b):
    """ This returns the the next vector approximation based on the formula: 
    X_k = D^(-1) (L+U)X^(k-1) + D^(-1)b, from the jacobi method.
    X_new = D^(-1)(b - (L+U)X)
    """
    #Making functions a bit shorter
    add = lambda a,b: matrix_add(a,b)
    sub = lambda a,b: matrix_sub(a,b)
    mult = lambda a,b: matrix_multiply(a,b)

    second_factor = sub(b, mult(add(L,U),X))
    return mult(diagonal_inverse(D), second_factor)
#+end_src

#+RESULTS:

** Jacobi method
#+begin_src python :results output :session
def jacobi_method(A, b, approximate_solution,
                  tolerance=0.01, iteration=0, max_iterations=100,
                  debug = False):
    """ Given an n by n matrix A and an n by 1 vector b, this function 
    returns the solution x to the equation Ax = b, with the accuracy
    specified by the tolerance. If the tolerance is not met within
    'max_iterations' ammount of iterations, an exception will be raised.
    """
    if debug:
        print("Approximate solution:", approximate_solution)
    if iteration > max_iterations:
        raise RecursionError("<jacobi_method: max iterations exceded!>")

    b_approximate = matrix_multiply(A, approximate_solution)
    distance = euclidean_distance(b, b_approximate)
    if debug:
        print("Distance:", distance)
    if distance <= tolerance:
        return approximate_solution
    else:
        X = approximate_solution
        D, L, U = additive_decomposition(A)
        X_new = jacobi_next_value(X, D, L, U, b)
        return jacobi_method(A, b,
                             X_new,
                             tolerance, iteration + 1, max_iterations,
                             debug)
#+end_src


#+RESULTS:

** Problem 3
#+begin_src python :results output :session
def problem3():
    """ The solution to task 2. """
    A = [[5, -2, 3], [-3, 9, 1], [2, -1, -7]]
    b = [[-1], [2], [3]]
    result = jacobi_method(A,b, zero_matrix(b), debug= False)
    print("A:", A)
    print("x from jacobi_method:", result)
    x_from_gaussian_elimination = [[ 59/317 ], [ 105/317 ], [ -134/317 ]]
    print("x from gaussian elimination:", x_from_gaussian_elimination)
    print("b from approximation:", matrix_multiply(A,result))
    print("b:", b)
problem3()
#+end_src

#+RESULTS:
: A: [[5, -2, 3], [-3, 9, 1], [2, -1, -7]]
: x from jacobi_method: [[0.18632557879118725], [0.33116049382716045], [-0.42264908597183726]]
: x from gaussian elimination: [[0.1861198738170347], [0.3312302839116719], [-0.4227129337539432]]
: b from approximation: [[-0.9986403516138964], [1.9988186220990454], [3.0000342655580745]]
: b: [[-1], [2], [3]]

* Gradien Descent 


** Gradient Descent method
#+begin_src python :results output :session
def gradient_descent(function, gradient_function, vector_approximation,
                     tolerance = 0.01, learning_step_size = 1,
                     iteration=0, max_iterations=100,
                     debug=False):
    """Finds a local minimum of "function" using the gradient descent method."""
    if debug:
        print(f"Iteration {iteration} vector: {vector_approximation}")
        print(f"Iteration {iteration} value: {function(vector_approximation)}")
    gradient = gradient_function(vector_approximation)
    if iteration >= max_iterations:
        raise RecursionError("<gradient_descent: max_iterations exceeded!>")
    if norm(gradient) <= tolerance:
        return function(vector_approximation), vector_approximation
    else:
        scaled_gradient = scale(learning_step_size, gradient)
        new_approximation = matrix_sub(vector_approximation, scaled_gradient)
        return gradient_descent(function, gradient_function, new_approximation,
                                tolerance, learning_step_size, 
                                iteration+1, max_iterations,
                                debug)
#+end_src

#+RESULTS:

** Problem 3
#+begin_src python :results output :session
def problem3():
    """ Solution to problem 3. """
    a,b = 1,5
    f_tupled = lambda x1, x2: (a-x1) ** 2 + b * (x2 - x1 ** 2) **2
    gradient_tupled = lambda x1, x2: [[-4 * b * x1 * (x2 - x1 ** 2) - 2 * (a-x1)],
                                       [2 * b * (x2 - x1 ** 2) ]]
    f = lambda X: f_tupled(X[0][0], X[1][0])
    gradient = lambda X: gradient_tupled(X[0][0], X[1][0])
    start_vector = [[ -1.4 ], [ 2 ]]

    result = gradient_descent(f, gradient, start_vector,
                              learning_step_size = 0.01,
                              debug=True)
    print(result)
problem3()
#+end_src

#+RESULTS:
#+begin_example
Iteration 0 vector: [[-1.4], [2]]
Iteration 1 vector: [[-1.3632], [1.996]]
Iteration 2 vector: [[-1.3534746456064], [1.982231424]]
Iteration 3 vector: [[-1.3471008348940428], [1.967197643229937]]
Iteration 4 vector: [[-1.3412499694542104], [1.951945944844166]]
Iteration 5 vector: [[-1.3354657341744396], [1.9366464984158416]]
Iteration 6 vector: [[-1.3296691524424107], [1.9213287212896648]]
Iteration 7 vector: [[-1.3238457302785103], [1.9059978546563903]]
Iteration 8 vector: [[-1.3179924350005834], [1.8906548209484155]]
Iteration 9 vector: [[-1.3121083349020217], [1.8752997447254507]]
Iteration 10 vector: [[-1.306192889483421], [1.8599325985048412]]
Iteration 11 vector: [[-1.3002456268063418], [1.844553325108062]]
Iteration 12 vector: [[-1.2942660805028194], [1.8291618616001575]]
Iteration 13 vector: [[-1.2882537769007847], [1.8137581441541548]]
Iteration 14 vector: [[-1.2822082321071742], [1.798342109108653]]
Iteration 15 vector: [[-1.2761289511196352], [1.7829136932461283]]
Iteration 16 vector: [[-1.2700154273550228], [1.7674728339100854]]
Iteration 17 vector: [[-1.2638671422573553], [1.7520194690910529]]
Iteration 18 vector: [[-1.2576835649122895], [1.736553537509725]]
Iteration 19 vector: [[-1.2514641516515217], [1.7210749787038009]]
Iteration 20 vector: [[-1.2452083456428362], [1.705583733120307]]
Iteration 21 vector: [[-1.2389155764642559], [1.6900797422141332]]
Iteration 22 vector: [[-1.2325852596613573], [1.6745629485532958]]
Iteration 23 vector: [[-1.2262167962869213], [1.6590332959314118]]
Iteration 24 vector: [[-1.2198095724220948], [1.6434907294878867]]
Iteration 25 vector: [[-1.2133629586781967], [1.6279351958363553]]
Iteration 26 vector: [[-1.20687630967827], [1.6123666432019506]]
Iteration 27 vector: [[-1.2003489635174283], [1.5967850215680195]]
Iteration 28 vector: [[-1.1937802412010017], [1.581190282832954]]
Iteration 29 vector: [[-1.187169446059435], [1.5655823809778509]]
Iteration 30 vector: [[-1.180515863138838], [1.5499612722457723]]
Iteration 31 vector: [[-1.1738187585660298], [1.5343269153334387]]
Iteration 32 vector: [[-1.1670773788868631], [1.5186792715962443]]
Iteration 33 vector: [[-1.1602909503765515], [1.503018305267563]]
Iteration 34 vector: [[-1.1534586783206562], [1.4873439836933788]]
Iteration 35 vector: [[-1.146579746265323], [1.4716562775833644]]
Iteration 36 vector: [[-1.1396533152352912], [1.455955161279613]]
Iteration 37 vector: [[-1.1326785229181142], [1.4402406130443308]]
Iteration 38 vector: [[-1.1256544828129642], [1.4245126153678938]]
Iteration 39 vector: [[-1.118580283342306], [1.4087711552987965]]
Iteration 40 vector: [[-1.1114549869246397], [1.3930162247971323]]
Iteration 41 vector: [[-1.1042776290064324], [1.377247821113384]]
Iteration 42 vector: [[-1.0970472170512562], [1.3614659471944526]]
Iteration 43 vector: [[-1.0897627294840704], [1.3456706121189979]]
Iteration 44 vector: [[-1.082423114588473], [1.3298618315643551]]
Iteration 45 vector: [[-1.0750272893546629], [1.3140396283074607]]
Iteration 46 vector: [[-1.0675741382757347], [1.298204032762438]]
Iteration 47 vector: [[-1.0600625120898426], [1.282355083557712]]
Iteration 48 vector: [[-1.052491226465646], [1.2664928281557635]]
Iteration 49 vector: [[-1.0448590606283537], [1.250617323518903]]
Iteration 50 vector: [[-1.0371647559235717], [1.2347286368247294]]
Iteration 51 vector: [[-1.0294070143160567], [1.2188268462352567]]
Iteration 52 vector: [[-1.0215844968203645], [1.2029120417240409]]
Iteration 53 vector: [[-1.0136958218602916], [1.1869843259660084]]
Iteration 54 vector: [[-1.0057395635539066], [1.1710438152951088]]
Iteration 55 vector: [[-0.9977142499208742], [1.1550906407353583]]
Iteration 56 vector: [[-0.989618361008708], [1.1391249491113398]]
Iteration 57 vector: [[-0.9814503269345066], [1.123146904244762]]
Iteration 58 vector: [[-0.9732085258386874], [1.1071566882442707]]
Iteration 59 vector: [[-0.9648912817471992], [1.0911545028963547]]
Iteration 60 vector: [[-0.9564968623386869], [1.0751405711658946]]
Iteration 61 vector: [[-0.9480234766131146], [1.0591151388156803]]
Iteration 62 vector: [[-0.9394692724584118], [1.043078476155074]]
Iteration 63 vector: [[-0.9308323341118254], [1.0270308799289203]]
Iteration 64 vector: [[-0.922110679512824], [1.010972675358835]]
Iteration 65 vector: [[-0.9133022575446405], [0.9949042183501118]]
Iteration 66 vector: [[-0.9044049451618548], [0.9788258978787143]]
Iteration 67 vector: [[-0.8954165444018344], [0.9627381385741647]]
Iteration 68 vector: [[-0.8863347792783806], [0.9466414035156004]]
Iteration 69 vector: [[-0.8771572925565959], [0.930536197259886]]
Iteration 70 vector: [[-0.8678816424088198], [0.9144230691224192]]
Iteration 71 vector: [[-0.8585052989525003], [0.8983026167332003]]
Iteration 72 vector: [[-0.84902564067212], [0.8821754898928325]]
Iteration 73 vector: [[-0.8394399507288101], [0.8660423947554196]]
Iteration 74 vector: [[-0.8297454131631135], [0.8499040983678363]]
Iteration 75 vector: [[-0.8199391089985589], [0.8337614335975753]]
Iteration 76 vector: [[-0.8100180122563304], [0.8176153044843528]]
Iteration 77 vector: [[-0.799978985894451], [0.8014666920538872]]
Iteration 78 vector: [[-0.78981877768862], [0.7853166606357699]]
Iteration 79 vector: [[-0.7795340160762444], [0.7691663647311475]]
Iteration 80 vector: [[-0.7691212059904154], [0.7530170564800286]]
Iteration 81 vector: [[-0.758576724716709], [0.7368700937824408]]
Iteration 82 vector: [[-0.7478968178129025], [0.7207269491323898]]
Iteration 83 vector: [[-0.7370775951401555], [0.7045892192286174]]
Iteration 84 vector: [[-0.7261150270641021], [0.6884586354315152]]
Iteration 85 vector: [[-0.7150049408958645], [0.6723370751411939]]
Iteration 86 vector: [[-0.7037430176564669], [0.6562265741776244]]
Iteration 87 vector: [[-0.6923247892637963], [0.640129340249885]]
Iteration 88 vector: [[-0.6807456362594269], [0.6240477676078124]]
Iteration 89 vector: [[-0.6690007862136432], [0.6079844529756564]]
Iteration 90 vector: [[-0.6570853129712608], [0.591942212873538]]
Iteration 91 vector: [[-0.6449941369287472], [0.5759241024384382]]
Iteration 92 vector: [[-0.6327220265651484], [0.5599334358618403]]
Iteration 93 vector: [[-0.6202636014858879], [0.5439738085657271]]
Iteration 94 vector: [[-0.6076133372800997], [0.5280491212419788]]
Iteration 95 vector: [[-0.5947655725392658], [0.512163605881847]]
Iteration 96 vector: [[-0.5817145184379716], [0.49632185392145833]]
Iteration 97 vector: [[-0.5684542713369649], [0.48052884662546463]]
Iteration 98 vector: [[-0.5549788289346386], [0.46478998782304215]]
Iteration 99 vector: [[-0.5412821105656526], [0.4491111390973042]]
Iteration 100 vector: [[-0.5273579823244602], [0.4334986575094145]]
#+end_example

* Test

** Transpose av vector
#+begin_src python :results output :session :tangle no
vector = [[1], [2], [3]]
print(transpose(vector))
#+end_src

#+RESULTS:
: [[1, 2, 3]]

