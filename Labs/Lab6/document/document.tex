% Created 2021-04-30 Fri 21:09
% Intended LaTeX compiler: pdflatex
\documentclass[10pt]{article}
     \usepackage{/home/john/texstuff/NoTeX/NotesTeXSW}
     \input{/home/john/skola/test/test3/bold.tex}
     \usepackage{minted}
\author{John Möller}
\date{\today}
\title{MM5016 Lab Assignment 6 (QR method)}
\hypersetup{
 pdfauthor={John Möller},
 pdftitle={MM5016 Lab Assignment 6 (QR method)},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.2 (Org mode 9.4.4)}, 
 pdflang={Samin}}
\begin{document}

\maketitle

\section{Task 1}
\label{sec:orge141a0b}
\begin{exercise}[1]  \label{exe:1}
Find a QR decomposition for the matrix based on the Gram-Schmidt method.
\begin{align*}
A=
\left( \begin{array}{c c c}
0  &  1  &  1 \\
1  &  1  &  2 \\
0  &  0  &  3
\end{array} \right)
.
\end{align*}

\end{exercise}
\begin{solution}[1]  \label{sol:1}
To QR decompose \(A\) we will interpret the columns of \(A\)
as three column vectors \(A = ( a_1 , a_2, a_3)\) expressed in an orthonormal
basis \(\beta\). By the Gram-Schmidt method we will find an orthonormal
basis  
\(\gamma\) such that if \(r_1, r_2 , r_3\) are the respective vectors \(a_1, a_2 , a_3\)
in the basis \(\gamma\), then the matrix \(R = (r_1, r_2, r_3)\) we be an upper
triangular matrix.

By the Gram-Schmidt method we will have an orthogonal basis and add our
new vector by removing all components that are unorthogonal relative to
the given orthogonal basis. We will get these components by projecting
our new vector on the vectors in the orthogonal basis. Then we can add
an arbitrary ammount of new vectors by using this defintion recursively.
When we're done we will normalise all our vectors in the basis and thus
it will be our orthonormal basis \(\gamma\).

So let \(v_k\) denote the orthogonal basis vectors we are looking for
that we will in the end normalise to get the orthonormal basis.
So we will start by simply letting \(v_1 = a_1\) as the set
\(\{ v_1 \}\) is orthogonal by definition.

We will get \(v_2\) by removing the \(v_1\) parallel component of \(a_2\):
\begin{align*}
v_2  &  = a_2 - \text{proj} _{v_1} (a_2) \\
& = a_2 - \frac{\left< v_1, a_2 \right> }{|v_1| ^2} v_1 \\
& = a_2 - v_1 \\
& = a_2 - a_1 \\
& = (1,1,0)^{t} - (0,1,0)^{t} \\
& = (1,0,0)^{t} 
.
\end{align*}

From this we can also conclude that \(a_2 = v_1 + v_2\) which we will
use when determinening \(r_2\).


We will do the same process with \(a_3\) to obtain \(v_3\), but this
time we need to remove both the \(v_1\) and \(v_2\) parallel components:
\begin{align*}
v_3  &  = a_3 - \text{proj} _{v_1}(a_3) - \text{proj} _{v_2}(a_3) \\
& = a_3 - \frac{\left< v_1, a_3 \right> }{| v_1 | ^2} v_1
- \frac{\left< v_2, a_3 \right> }{| v_2 | ^2} v_2 \\
& = a_3 - 2v_1 - v_2 \\
& = (1,2,3)^{t} - 2(0,1,0)^{t} - (1,0,0)^{t} \\
& = (0,0,3)^{t} 
.
\end{align*}
From this we can also conclude that \(a_3 = 2v_1 + v_2 + v_3\), which we will
use when determining \(r_3\).

So now we have an orthogonal basis
\begin{align*}
\{ v_1, v_2, v_3 \} = \{
\left( \begin{array}{c}
0 \\ 1 \\ 0
\end{array} \right)
\left( \begin{array}{c}
1 \\ 0 \\ 0
\end{array} \right)
\left( \begin{array}{c}
0 \\ 0 \\ 3
\end{array} \right)
 \} 
.
\end{align*}

As each vector is has a single coordinate we can easily see
that we can normalise it my dividing by \(v_3\) by 3. If we
denote our orthonormal basis \(\gamma\) as,
\(\gamma = \{ q_1, q_2, q_3 \}\), then \(q_1 = v_1\), \(q_2 = v_2\) and \(q_3 = \frac{v_3}{3}\).

As \(r_k\) are \(a_k\) in the basis \(\gamma\), we know have enough information to
determine \(r_k\). \(a_1 = v_1 = q_1\), so \(r_1 = (1,0,0)^{t}\).
\(a_2 = v_1 + v_2 = q_1 + q_2\), so \(r_2 = (1,1,0)^{t}\). And lastly
we have that \(a_3 = 2v_1 + v_2 + v_3 = 2q_1 + q_2 + 3q_3\), so
\(r_3 = (2,1,3)^{t}\).


If we had a vector \(v\) in the basis \(\gamma\) and we wanted to express it
in the basis \(\beta\), we could write \(Qv\) where \(Q\) is the matrix
\(Q = (q_1, q_2, q_3)\). \(v\) doesn't need to be a single column vector,
it can also be a row vector of column vectors, and by multiplying
\(Q\) on the left we get a new row vector where each row column vector
has changed basis. Thus we have that \(A = QR\), where
\(R = (r_1, r_2, r_3)\).

We can explicitly write out the matrices:
\begin{align*}
A =
\left( \begin{array}{c c c}
0  &  1  &  0 \\
1  &  0  &  0 \\
0  &  0  &  1
\end{array} \right)
\left( \begin{array}{c c c}
1  &  1  &  2 \\
0  &  1  &  1 \\
0  &  0  &  3
\end{array} \right)
.
\end{align*}


To control that \(Q\) is orthonormal we will verify that \(\left< q_i, q_j \right> = \delta _{ij}\),
where  w\(\delta _{ij}\) is the kronecker delta function.
If we define the matrix \(M _{ij} = \left< q_i , q_j \right>\), then the statement that Q is orthonormal
is equivalent to saying that \(M = I\), where \(I\) is the \(3 \times 3\) identity matrix.

Let us denote the i:th component of a vector \(v_k\) in the basis \(\beta\), as
\(v _{k}^{i}\). Let us consider the matrix \(Q^{t} Q\). It is defined as
\begin{align*}
(Q^{t} Q) _{ij}  &  = \sum_{ k = 0 }^{ n } (Q^{t}) _{ik} Q _{kj} \\
& = \sum_{ k = 0 }^{ n } Q _{ki} Q _{kj} \\
& = \sum_{ k = 0 }^{ n } q _{i}^{k} q _{j}^{k} \\
& = \left< q_i , q_j \right> 
.
\end{align*}

We can now see that \(M = Q^{t} Q\), which means that the statement that
\(M = I\) is equivalent to that \(Q^{t} Q = I\), which is equivalent to
\begin{align*}
\left( \begin{array}{c c c}
0  &  1  &  0 \\
1  &  0  &  0 \\
0  &  0  &  1
\end{array} \right)
\left( \begin{array}{c c c}
0  &  1  &  0 \\
1  &  0  &  0 \\
0  &  0  &  1
\end{array} \right)
=
\left( \begin{array}{c c c}
1  &  0  &  0 \\
0  &  1  &  0 \\
0  &  0  &  1
\end{array} \right)
.
\end{align*}

And because the last statement is true, that means that our original
statement that \(\gamma\) is an orthonormal basis is true.
\end{solution}
\end{document}